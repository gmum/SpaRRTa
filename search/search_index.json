{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"SpaRRTa <p> Spatial Relation Recognition Task       A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models     </p> \ud83d\ude80 Get Started        \ud83d\udce6 GitHub        \ud83d\udcc4 Paper        \ud83d\udcca Dataset (coming soon)"},{"location":"#abstract","title":"Abstract","text":"<p>Visual Foundation Models (VFMs), such as DINO and CLIP, exhibit strong semantic understanding but show limited spatial reasoning capabilities, which limits their applicability to embodied systems. Recent work incorporates 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across different tasks, raising the question: do these models truly have spatial awareness or overfit to specific 3D objectives?</p> <p>To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the representations of relative positions of objects across different viewpoints. SpaRRTa can generate an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations.</p> \ud83c\udfae Unreal Engine 5        Photorealistic synthetic scenes with full control over object placement, camera positions, and environmental conditions.      \ud83d\udd2c Spatial Reasoning        Evaluates abstract, human-like relational understanding beyond simple depth estimation or metric prediction.      \ud83d\udc41\ufe0f Egocentric &amp; Allocentric        Two task variants testing camera-centric and perspective-taking spatial reasoning abilities.      \ud83d\udcca Comprehensive Benchmark        Evaluate 13+ VFMs across 5 diverse environments with multiple probing strategies."},{"location":"#key-statistics","title":"Key Statistics","text":"5 Environments 13+ VFMs Evaluated 50K+ Images 3 Probing Methods"},{"location":"#key-findings","title":"Key Findings","text":"<p>Main Results</p> <ol> <li> <p>Spatial information is patch-level: Spatial relations are primarily encoded at the patch level and largely obscured by global pooling</p> </li> <li> <p>3D supervision enriches patch features: VGGT (3D-supervised) shows improvements only with selective probing, not linear probing</p> </li> <li> <p>Allocentric reasoning is challenging: All models struggle with perspective-taking tasks compared to egocentric variants</p> </li> <li> <p>Environment complexity matters: Performance degrades significantly in cluttered environments like City scenes</p> </li> </ol>"},{"location":"#environments","title":"Environments","text":"\ud83c\udfd4\ufe0f Winter Town \ud83c\udfd9\ufe0f City \ud83c\udf32 Forest \ud83c\udfdc\ufe0f Desert <p>View All Environments \u2192</p>"},{"location":"#evaluation-pipeline","title":"Evaluation Pipeline","text":"The SpaRRTa evaluation pipeline: (1) Set Stage with diverse assets, (2) Set Camera position, (3) Render photorealistic image, (4) Extract ground truth, (5) Run VFM and probe, (6) Calculate accuracy."},{"location":"#authors","title":"Authors","text":"Turhan Can Kargin Jagiellonian University \u2709\ufe0f \ud83d\udcbb Wojciech Jasi\u0144ski Jagiellonian University, AGH University of Krakow Adam Pardyl Jagiellonian University, IDEAS NCBR Bartosz Zieli\u0144ski Jagiellonian University Marcin Przewi\u0119\u017alikowski Jagiellonian University"},{"location":"#affiliations","title":"Affiliations","text":""},{"location":"#citation","title":"Citation","text":"<p>If you find SpaRRTa useful in your research, please cite our paper:</p> \ud83d\udccb Copy BibTeX <pre><code>@misc{kargin2026sparrta,\n  title={SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models},\n  author={Turhan Can Kargin and Wojciech Jasi\u0144ski and Adam Pardyl and Bartosz Zieli\u0144ski and Marcin Przewi\u0119\u017alikowski},\n  year={2026},\n  eprint={2601.11729},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV},\n  url={https://arxiv.org/abs/2601.11729}\n}</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This research has been supported by the flagship project entitled \u201cArtificial Intelligence Computing Center Core Facility\u201d from the Priority Research Area DigiWorld under the Strategic Programme Excellence Initiative at the Jagiellonian University. The work of Turhan Can Kargin, Adam Pardyl, and Bartosz Zieli\u0144ski was supported by National Science Centre (Poland) grant number 2023/50/E/ST6/00469. The research of Marcin Przewi\u0119\u017alikowski was supported by the National Science Centre (Poland), grant no. 2023/49/N/ST6/03268. We gratefully acknowledge Polish high-performance computing infrastructure PLGrid (HPC Center: ACK Cyfronet AGH) for providing computer facilities and support within computational grant no. PLG/2025/018312.</p> Get Started with SpaRRTa View Results"},{"location":"evaluation-vfms/","title":"Evaluation of VFMs","text":""},{"location":"evaluation-vfms/#evaluation-of-visual-foundation-models","title":"Evaluation of Visual Foundation Models","text":"<p>SpaRRTa provides a systematic methodology for evaluating how Visual Foundation Models (VFMs) encode and represent spatial relations between objects.</p>"},{"location":"evaluation-vfms/#the-spatial-relation-recognition-task","title":"The Spatial Relation Recognition Task","text":"<p>The core task is to determine the relative spatial relation between two objects in an image:</p> <ul> <li>Source Object: The reference point for the spatial relation</li> <li>Target Object: The object whose position is being queried</li> <li>Viewpoint: The perspective from which the relation is evaluated</li> </ul> <p>Left: Egocentric task (camera viewpoint). Right: Allocentric task (human viewpoint).</p>"},{"location":"evaluation-vfms/#task-variants","title":"Task Variants","text":"Egocentric (SpaRRTa-ego)Allocentric (SpaRRTa-allo) <p>The camera defines the viewpoint for spatial relations.</p> <p>Example Query</p> <p>\"Where is the tree (target) relative to the car (source) from the camera's perspective?\"</p> <p>Characteristics:</p> <ul> <li>Directly observable from input image</li> <li>Simpler\u2014no perspective transformation needed</li> <li>Tests basic spatial layout understanding</li> </ul> <p>Answer: The relation as seen from the camera (Front/Back/Left/Right)</p> <p>A third object (human) defines the viewpoint for spatial relations.</p> <p>Example Query</p> <p>\"Where is the tree (target) relative to the car (source) from the human's perspective?\"</p> <p>Characteristics:</p> <ul> <li>Requires implicit perspective transformation</li> <li>More challenging\u2014must reason from another viewpoint</li> <li>Tests abstract spatial reasoning capability</li> </ul> <p>Answer: The relation as would be seen from the human's position</p>"},{"location":"evaluation-vfms/#classification-labels","title":"Classification Labels","text":"<p>The task is formulated as a 4-way classification:</p> Label Description Front Target is in front of source (from viewpoint) Back Target is behind source (from viewpoint) Left Target is to the left of source (from viewpoint) Right Target is to the right of source (from viewpoint)"},{"location":"evaluation-vfms/#evaluated-models","title":"Evaluated Models","text":"<p>We evaluate a diverse suite of VFMs spanning different learning paradigms:</p>"},{"location":"evaluation-vfms/#joint-embedding-architectures-jea","title":"Joint-Embedding Architectures (JEA)","text":"Model Backbone Pre-training Dataset DINO ViT-B/16 Contrastive / Distillation ImageNet-1k DINO-v2 ViT-B/14 DINO + iBOT LVD-142M DINO-v2 (+reg) ViT-B/14, ViT-L/14 DINO-v2 w/ Register Tokens LVD-142M DINOv3 ViT-B/16 DINO + iBOT LVD-1689M"},{"location":"evaluation-vfms/#masked-image-modeling-mim","title":"Masked Image Modeling (MIM)","text":"Model Backbone Pre-training Dataset MAE ViT-B/16 Pixel Reconstruction ImageNet-1k MaskFeat ViT-B/16 HOG Feature Prediction ImageNet-1k SPA ViT-B/16 Masked Volumetric Neural Rendering ScanNet, Hypersim, S3DIS CroCo ViT-B/16 Cross-View Completion Habitat CroCov2 ViT-B/16 Cross-View Completion ARKitScenes, MegaDepth, ..."},{"location":"evaluation-vfms/#supervised-weakly-supervised","title":"Supervised &amp; Weakly Supervised","text":"Model Backbone Pre-training Dataset VGGT ViT-L/14 Multi-Task 3D Regression Co3D, MegaDepth, etc. DeiT ViT-B/16 Classification + Distillation ImageNet-1k CLIP ViT-B/16 Image-Text Contrastive Web Image-Text (WIT)"},{"location":"evaluation-vfms/#interactive-prediction-demo","title":"Interactive Prediction Demo","text":"<p>Explore spatial relation predictions from the VGGT model using our interactive demo. Efficient Probing is used for prediction. Attention maps of efficient probing heads are visualized to show which regions of the image the model is paying attention to. Select an environment and viewpoint to see how the model predicts spatial relations between objects.</p> Environment: Bridge Forest City Desert Winter Town Viewpoint: Camera (Egocentric) Human (Allocentric) Original Image VGGT Prediction \ud83d\udd2e       Predict      <p>Reference Object: Tree</p> <p>Target Object: Car</p> <p>\ud83d\udca1 Hover over the images to see objects relationships!</p>"},{"location":"evaluation-vfms/#probing-methodology","title":"Probing Methodology","text":"<p>We evaluate frozen VFM representations using lightweight probing heads:</p> <p>Three probing strategies: Linear Probing with GAP, AbMILP, and Efficient Probing.</p>"},{"location":"evaluation-vfms/#probing-strategies","title":"Probing Strategies","text":"Linear Probing (GAP)AbMILPEfficient Probing <p>Global Average Pooling + Linear Classifier</p> <pre><code>dataloader = DataLoader(dataset, batch_size=batch_size)\nfor images in dataloader:\n    features = vfm.forward_features(images)  # [Batch_size, Num_patches, Dimension]\n    global_feat = features.mean(dim=1)     # [Batch_size, Dimension]\n    prediction = linear_layer(global_feat)  # [Batch_size, 4]\n</code></pre> <p>Pros: Simple baseline, standard evaluation protocol</p> <p>Cons: Treats all patches equally, loses local spatial information</p> <p>Attention-Based Multiple Instance Learning Pooling</p> <pre><code>dataloader = DataLoader(dataset, batch_size=batch_size)\nfor images in dataloader:\n  features = vfm.forward_features(images)  # [Batch_size, Num_patches, Dimension]\n  attn_map = linear_layer(features, 1)        # [Batch_size, Num_patches, 1]\n  attn_map = softmax(attn_map, dim=1)      # [Batch_size, Num_patches, 1]\n  weighted_feat = (attn_map * features).sum(dim=1)  # [Batch_size, Dimension]\n  prediction = linear_layer(weighted_feat)   # [Batch_size, 4]\n</code></pre> <p>Pros: Learns to focus on relevant patches</p> <p>Cons: Single attention map may not capture multiple objects</p> <p>Multi-Query Cross-Attention</p> <pre><code>dataloader = DataLoader(dataset, batch_size=batch_size)\nqueries = Parameter(torch.randn(1, num_queries, dimension) * 0.02)  # [1, Num_queries, Dimension]\nvalues = linear_layer(dimension, (dimension/d_out)/num_queries)\nfor images in dataloader:\n  features = vfm.forward_features(images)  # [Batch_size, Num_patches, Dimension]\n  attn_map = queries @ features.transpose(-2, -1)  # [Batch_size, Num_queries, Num_patches]\n  attn_map = softmax(attn_map, dim=-1)  # [Batch_size, Num_queries, Num_patches]\n  weighted_feat = matmul(attn_map, values)  # [Batch_size, Num_queries, (Dimension/d_out)/Num_queries]\n  weighted_feat = weighted_feat.view(Batch_size, -1)  # [Batch_size, Dimension/d_out]\n  prediction = linear_layer(weighted_feat)   # [Batch_size, 4]\n</code></pre> <p>Pros: Multiple queries can specialize to different objects/regions</p> <p>Cons: More parameters, may overfit on small datasets</p>"},{"location":"evaluation-vfms/#hyperparameters","title":"Hyperparameters","text":"Parameter Linear AbMILP Efficient Optimizer AdamW AdamW AdamW Scheduler Cosine Decay Cosine Decay Cosine Decay Learning Rate 1e-2, 1e-3, 1e-4 1e-2, 1e-3, 1e-4 1e-2, 1e-3, 1e-4 Weight Decay 0.001 0.001 0.001 Dropout 0.2, 0.4, 0.6 0.2, 0.4, 0.6 0.2, 0.4, 0.6 Batch Size 256 256 256 Epochs 1000 500 500 Warmup Steps 200 100 100 Queries (num_queries) - - 4 Output Dimension (d_out) D_i D_i D_i/8"},{"location":"evaluation-vfms/#evaluation-protocol","title":"Evaluation Protocol","text":""},{"location":"evaluation-vfms/#data-splits","title":"Data Splits","text":"<p>For each environment and object triple:</p> <ul> <li>Training: 80%</li> <li>Validation: 10% (hyperparameter selection)</li> <li>Test: 10% (final evaluation)</li> </ul>"},{"location":"evaluation-vfms/#metrics","title":"Metrics","text":"<ul> <li>Accuracy: Primary metric (4-way classification)</li> <li>Mean Rank: Model ranking across environments/tasks</li> <li>Per-Environment Accuracy: Fine-grained analysis</li> </ul>"},{"location":"evaluation-vfms/#reproducibility","title":"Reproducibility","text":"<ul> <li>Random Seeds: 2 seeds per experiment</li> <li>Object Triples: 3 distinct triples per environment</li> <li>Cross-Validation: Validation set for best checkpoint selection</li> </ul>"},{"location":"evaluation-vfms/#key-insights","title":"Key Insights","text":""},{"location":"evaluation-vfms/#performance-hierarchy","title":"Performance Hierarchy","text":"<pre><code>graph LR\n    A[Linear Probing] --&gt;|\"worse than\"| B[AbMILP]\n    B --&gt;|\"worse than\"| C[Efficient Probing]\n\n    style A fill:#ff6b6b,color:#fff\n    style B fill:#4a90e2,color:#fff\n    style C fill:#1dd1a1,color:#fff</code></pre> <p>Main Finding</p> <p>Spatial information is primarily encoded at the patch level and is largely obscured by global pooling. Selective probing mechanisms (AbMILP, Efficient Probing) consistently outperform linear probing.</p>"},{"location":"evaluation-vfms/#model-rankings","title":"Model Rankings","text":"Impact of probing strategy on spatial accuracy across all VFMs. <p>Top Performers:</p> <ol> <li>VGGT (with Efficient Probing) - Best overall spatial reasoning</li> <li>DINO-v2 (+reg) ViT-L - Strong across all probing methods</li> <li>DINOv3 - Best ViT-B model with Efficient Probing</li> <li>MAE - Surprisingly strong performance</li> </ol> <p>Underperformers:</p> <ul> <li>CLIP and DeiT - Limited spatial awareness, their semantic features don't transfer to spatial tasks</li> </ul>"},{"location":"evaluation-vfms/#task-difficulty","title":"Task Difficulty","text":"Egocentric vs Allocentric performance comparison across all VFMs. <p>Allocentric Challenge</p> <p>All models show significant performance drops on allocentric tasks compared to egocentric. This indicates that perspective-taking remains a fundamental challenge for current VFMs.</p> View Full Results \u2192 Get Started"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#examples","title":"Examples","text":"<p>This page provides practical code examples for using SpaRRTa in your research.</p> <p>Coming Soon</p> <p>This section is currently under development. Full documentation will be available soon.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#getting-started","title":"Getting Started","text":"<p>This guide will help you set up SpaRRTa for evaluating Visual Foundation Models on spatial reasoning tasks.</p> <p>Coming Soon</p> <p>This section is currently under development. Full documentation will be available soon.</p>"},{"location":"results/","title":"More Results","text":""},{"location":"results/#benchmark-results","title":"Benchmark Results","text":"<p>This page presents the complete evaluation results of Visual Foundation Models on the SpaRRTa benchmark.</p>"},{"location":"results/#leaderboard","title":"Leaderboard","text":""},{"location":"results/#overall-rankings","title":"Overall Rankings","text":"<p>The table below shows the mean rank across all environments and probing methods:</p> Rank Model Mean Rank (Ego) Mean Rank (Allo) Overall \ud83e\udd47 DINO-v2 (+reg) ViT-L/14 1.00 1.20 1.10 \ud83e\udd48 VGGT ViT-L/14 2.40 1.20 1.80 \ud83e\udd49 DINOv3 2.80 2.60 2.70 4 DINO-v2 ViT-B/14 5.00 4.20 4.60 5 MAE 5.00 8.00 6.50 6 DINO-v2 (+reg) ViT-B/14 3.00 4.00 3.50 7 DINO 4.80 3.20 4.00 8 CroCo v2 9.00 10.40 9.70 9 CroCo 10.80 11.20 11.00 10 SPA 9.00 10.00 9.50 11 MaskFeat 11.40 12.00 11.70 12 DeiT 9.20 5.80 7.50 13 CLIP 13.00 11.40 12.20"},{"location":"results/#egocentric-task-results","title":"Egocentric Task Results","text":""},{"location":"results/#complete-results-table","title":"Complete Results Table","text":"Impact of probing strategy on spatial accuracy across all VFMs. ForestDesertWinter TownBridgeCity Model Linear AbMILP Efficient DINO 64.76 85.53 89.28 DINO-v2 (B/14) 59.91 86.36 91.91 DINO-v2 reg (B/14) 60.81 83.81 91.60 DINO-v2 reg (L/14) 65.37 86.59 93.92 DINOv3 62.41 84.71 93.93 VGGT (L/14) 52.10 89.90 96.18 SPA 52.91 73.01 78.33 CroCo 46.25 75.19 88.98 CroCo v2 54.14 76.18 90.48 CLIP 36.66 55.85 56.33 DeiT 53.23 61.33 75.49 MAE 62.82 84.26 93.10 MaskFeat 47.20 74.60 89.66 Model Linear AbMILP Efficient DINO 55.14 89.34 92.10 DINO-v2 (B/14) 64.57 90.64 93.94 DINO-v2 reg (B/14) 69.40 86.73 94.63 DINO-v2 reg (L/14) 79.68 89.03 96.70 DINOv3 65.34 86.96 97.47 VGGT (L/14) 63.42 90.03 98.78 SPA 49.39 72.58 90.80 CroCo 48.77 80.75 89.88 CroCo v2 52.53 81.91 90.88 CLIP 37.65 56.29 68.44 DeiT 54.75 52.45 78.22 MAE 58.36 87.34 93.71 MaskFeat 42.87 81.52 92.87 Model Linear AbMILP Efficient DINO 61.97 87.05 89.32 DINO-v2 (B/14) 58.03 88.41 93.26 DINO-v2 reg (B/14) 65.38 86.59 91.52 DINO-v2 reg (L/14) 69.02 85.15 94.92 DINOv3 66.14 83.56 93.18 VGGT (L/14) 59.24 87.24 95.53 SPA 49.62 74.01 90.89 CroCo 49.32 83.17 87.95 CroCo v2 53.56 85.20 89.62 CLIP 44.09 61.57 63.21 DeiT 54.24 62.42 75.98 MAE 62.80 85.84 93.26 MaskFeat 47.80 78.63 91.21 Model Linear AbMILP Efficient DINO 63.34 89.48 89.86 DINO-v2 (B/14) 66.69 89.63 93.75 DINO-v2 reg (B/14) 67.15 88.49 93.52 DINO-v2 reg (L/14) 69.59 89.40 96.11 DINOv3 67.29 87.11 94.82 VGGT (L/14) 58.92 88.49 96.11 SPA 56.90 77.06 88.72 CroCo 55.57 81.86 92.68 CroCo v2 56.25 83.52 91.01 CLIP 45.35 61.13 64.71 DeiT 56.17 64.10 81.40 MAE 62.50 88.11 93.83 MaskFeat 53.89 82.85 92.30 Model Linear AbMILP Efficient DINO 53.01 83.89 82.45 DINO-v2 (B/14) 53.24 85.31 93.00 DINO-v2 reg (B/14) 57.15 82.98 93.00 DINO-v2 reg (L/14) 64.79 82.38 94.27 DINOv3 56.93 81.63 92.40 VGGT (L/14) 46.08 83.99 94.47 SPA 45.48 69.75 80.27 CroCo 44.88 74.93 87.95 CroCo v2 43.90 77.93 85.99 CLIP 42.92 62.58 64.67 DeiT 43.38 55.50 76.50 MAE 52.87 78.77 89.00 MaskFeat 44.80 72.67 77.64"},{"location":"results/#allocentric-task-results","title":"Allocentric Task Results","text":"Egocentric vs Allocentric performance comparison across all VFMs. <p>Performance Gap</p> <p>All models show a significant performance drop on the allocentric task compared to egocentric. This highlights the challenge of perspective-taking for current VFMs.</p>"},{"location":"results/#allocentric-performance-summary","title":"Allocentric Performance Summary","text":"Model Avg Linear Avg AbMILP Avg Efficient DINO 49.07 62.11 64.38 DINO-v2 (B/14) 47.90 65.91 71.20 DINO-v2 reg (B/14) 48.23 63.80 68.06 DINO-v2 reg (L/14) 51.68 66.86 76.10 DINOv3 49.51 60.87 72.05 VGGT (L/14) 42.70 65.71 76.65 SPA 38.37 55.52 66.14 CroCo 37.89 61.88 68.37 CroCo v2 37.86 62.77 67.69 CLIP 36.77 51.40 54.36 DeiT 45.42 49.31 57.88 MAE 42.61 64.45 70.66 MaskFeat 35.23 59.26 69.13"},{"location":"results/#environment-analysis","title":"Environment Analysis","text":"Performance variation across different environments."},{"location":"results/#environment-difficulty-ranking","title":"Environment Difficulty Ranking","text":"<p>From easiest to hardest:</p> <ol> <li>Forest - Natural complexity but clear object boundaries</li> <li>Desert - Sparse, homogeneous, minimal occlusion</li> <li>Bridge - Mixed complexity, infrastructure elements</li> <li>Winter Town - Snow occlusion, village clutter</li> <li>City - Dense urban geometry, maximum visual complexity</li> </ol> <p>Key Insight</p> <p>Environmental complexity significantly affects performance. Models struggle more in cluttered environments (City, Winter Town) compared to sparse environments (Forest, Desert).</p>"},{"location":"results/#probing-method-comparison","title":"Probing Method Comparison","text":"Comparison of Linear Probing (CLS token) vs Efficient Probing."},{"location":"results/#performance-hierarchy","title":"Performance Hierarchy","text":"<pre><code>Linear Probing &lt; AbMILP &lt; Efficient Probing\n</code></pre> <p>Key Finding: Spatial information is primarily encoded at the patch level and is largely lost through global average pooling. Selective probing mechanisms consistently unlock hidden spatial capabilities.</p>"},{"location":"results/#attention-visualizations","title":"Attention Visualizations","text":""},{"location":"results/#efficient-probing-attention-maps","title":"Efficient Probing Attention Maps","text":"Mean attention maps from Efficient Probing heads on VGGT. Individual query attention maps showing specialization to different objects. <p>Observation: Different queries specialize to attend to different scene elements (source, target, viewpoint objects), enabling the probe to extract relational information.</p>"},{"location":"results/#correlation-with-other-benchmarks","title":"Correlation with Other Benchmarks","text":"Pearson correlation between SpaRRTa and other vision benchmarks."},{"location":"results/#key-correlations","title":"Key Correlations","text":"Benchmark SpaRRTa-ego SpaRRTa-allo Depth Estimation r = 0.62 r = 0.66 Camera Pose r = 0.62 r = 0.59 FGVC Aircraft r = -0.23 r = -0.02 Flowers102 r = -0.19 r = 0.02 <p>Validation</p> <p>SpaRRTa shows strong correlation with 3D geometric tasks (depth, pose) but no correlation with semantic classification tasks. This confirms that SpaRRTa measures spatial awareness as an independent capability from semantic understanding.</p>"},{"location":"results/#layer-wise-analysis","title":"Layer-wise Analysis","text":"Spatial reasoning accuracy across different transformer layers. <p>Finding: Spatial information peaks at late-intermediate layers (18-20 for ViT-L), not the final layer. This suggests that final layers prioritize semantic abstraction over geometric detail.</p>"},{"location":"results/#attention-dynamics-analysis","title":"Attention Dynamics Analysis","text":"Methodology for analyzing inter-object attention flow."},{"location":"results/#vggt-vs-dino-v2-comparison","title":"VGGT vs DINO-v2 Comparison","text":"DINO-v2 <ul> <li>Objects attend strongly to themselves</li> <li>CLS token retains object attention</li> <li>Linear probing works moderately well</li> </ul> VGGT <ul> <li>Objects increasingly attend to other objects</li> <li>CLS token shifts to register tokens</li> <li>Efficient probing unlocks hidden spatial info</li> </ul> Attention flow from the global [CLS] token (top row) and human patches (bottom row) to the rest of the patches (Human, Truck, Tree, Background, [CLS] and Register) for both DINO-v2 and VGGT backbones across transformer layers. The two columns on the left show absolute attention scores for DINO-v2 and VGGT, while the right column displays the differential (VGGT - DINOv2) to quantify the divergence in attention dynamics. View Code Examples \u2192 Get Started"},{"location":"unreal-scene-generation/","title":"Unreal Scene Generation","text":""},{"location":"unreal-scene-generation/#unreal-scene-generation","title":"Unreal Scene Generation","text":"<p>SpaRRTa leverages Unreal Engine 5 to generate photorealistic synthetic images with precise control over object placement, camera positions, and environmental conditions. This enables the creation of a rigorous benchmark with mathematically precise ground-truth labels.</p>"},{"location":"unreal-scene-generation/#why-synthetic-data","title":"Why Synthetic Data?","text":"\ud83c\udfaf Precise Control        Full control over object positions, camera angles, and scene composition enables mathematically rigorous ground-truth labels.      \ud83d\udcc8 Scalability        Generate arbitrary amounts of diverse data without expensive manual annotation or data collection.      \ud83c\udfa8 Photorealism        Unreal Engine 5's Lumen and Nanite technologies provide state-of-the-art visual fidelity.      \ud83d\udd04 Reproducibility        Fully deterministic generation enables exact reproduction of experimental conditions."},{"location":"unreal-scene-generation/#evaluation-environments","title":"Evaluation Environments","text":"<p>SpaRRTa includes five diverse high-fidelity environments to ensure robust evaluation across different visual domains:</p> <p> </p> The five SpaRRTa evaluation environments: Forest, Desert, Winter Town, Bridge, and City."},{"location":"unreal-scene-generation/#environment-details","title":"Environment Details","text":"\ud83c\udf32 Forest\ud83c\udfdc\ufe0f Desert\ud83c\udfd4\ufe0f Winter Town\ud83c\udf09 Bridge\ud83c\udfd9\ufe0f City <p>Electric Dreams Environment</p> <p>A sparse forest landscape with complex foliage, uneven terrain, and natural rock formations. This environment tests spatial reasoning in organic, unstructured settings.</p> <ul> <li>Source: Electric Dreams (Epic Games)</li> <li>Characteristics: Complex foliage, uneven terrain, natural lighting</li> <li>Objects: Bear, Fox, Tent, Rocks, Trees</li> </ul> <p>Arid Landscape</p> <p>A vast, arid landscape characterized by open terrain, sand dunes, and high-contrast lighting. This environment is sparse and texture-homogeneous.</p> <ul> <li>Source: Realistic Desert Pack</li> <li>Characteristics: Open terrain, high contrast lighting, minimal occlusion</li> <li>Objects: Camel, Barrel, Cactus, Rocks</li> </ul> <p>Eastern European Village</p> <p>A snow-covered setting reflecting a typical small Eastern European town with cold lighting, snow textures, and village buildings.</p> <ul> <li>Source: Russian Winter Town</li> <li>Characteristics: Cold lighting, snow textures, village architecture</li> <li>Objects: Husky, Deer, Snowman</li> </ul> <p>Valley Infrastructure</p> <p>A valley scene centered around a large bridge infrastructure with mixed natural and man-made elements.</p> <ul> <li>Source: Automotive Bridge Scene</li> <li>Characteristics: Infrastructure elements, valley terrain, mixed complexity</li> <li>Objects: Bicycle, Trash Can, Vehicle</li> </ul> <p>Modern Metropolis</p> <p>A large-scale, modern American metropolis featuring high-rise architecture, paved roads, and complex urban geometry.</p> <ul> <li>Source: City Sample (Epic Games)</li> <li>Characteristics: Dense urban geometry, complex occlusion, varied lighting</li> <li>Objects: Motorcycle, Traffic Cone, Fire Hydrant</li> </ul>"},{"location":"unreal-scene-generation/#asset-library","title":"Asset Library","text":"SpaRRTa's curated asset library spanning Animals, Vehicles, Nature, and Human categories."},{"location":"unreal-scene-generation/#asset-selection-criteria","title":"Asset Selection Criteria","text":"<p>Our asset selection follows specific criteria to ensure valid spatial reasoning evaluation:</p> <ol> <li>ImageNet Alignment: Objects align with common ImageNet super-categories to ensure VFMs can recognize them</li> <li>Isotropic Sources: Source objects (rocks, trees, cones) are rotationally symmetric to minimize orientation ambiguity</li> <li>Environmental Coherence: Objects naturally fit their respective environments (e.g., camels in desert)</li> <li>Visual Distinctiveness: Objects are clearly distinguishable from backgrounds and each other</li> </ol> Category Assets Animals Bear, Fox, Camel, Husky, Deer Vehicles Car, Taxi, Motorcycle, Bicycle Nature Trees, Rocks, Cactus Objects Tent, Barrel, Trash Can, Traffic Cone, Fire Hydrant, Snowman Humans Human agent (viewpoint for allocentric tasks)"},{"location":"unreal-scene-generation/#data-generation-pipeline","title":"Data Generation Pipeline","text":"The complete SpaRRTa data generation and evaluation pipeline."},{"location":"unreal-scene-generation/#pipeline-steps","title":"Pipeline Steps","text":"<pre><code>flowchart LR\n    A[Set Stage] --&gt; B[Set Camera]\n    B --&gt; C[Render View]\n    C --&gt; D[Get Ground Truth]\n    D --&gt; E[Run Model]\n    E --&gt; F[Calculate Results]\n\n    style A fill:#7c4dff,color:#fff\n    style B fill:#7c4dff,color:#fff\n    style C fill:#536dfe,color:#fff\n    style D fill:#536dfe,color:#fff\n    style E fill:#3f1dcb,color:#fff\n    style F fill:#3f1dcb,color:#fff</code></pre>"},{"location":"unreal-scene-generation/#1-set-stage","title":"1. Set Stage","text":"<p>The evaluator establishes the scene configuration:</p> <ul> <li>Select environment (Forest, Desert, Winter Town, Bridge, City)</li> <li>Choose source, target, and viewpoint objects from the asset library</li> <li>Randomly sample object positions from a Gaussian distribution</li> <li>Apply physics-aware terrain adaptation via raycasting</li> </ul>"},{"location":"unreal-scene-generation/#2-set-camera","title":"2. Set Camera","text":"<p>Configure the viewpoint for image capture:</p> <ul> <li>Sample camera position within a defined area surrounding scene center</li> <li>Orient camera toward placed objects</li> <li>Validate visibility constraints (objects within field of view)</li> <li>Ensure proper scene composition (no extreme clustering or distance)</li> </ul>"},{"location":"unreal-scene-generation/#3-render-view","title":"3. Render View","text":"<p>Generate high-fidelity imagery using Unreal Engine 5:</p> <ul> <li>Ray-traced RGB image with dynamic global illumination</li> <li>Ground-truth segmentation masks for validation</li> <li>Resolution: 2048x2048 - Later, it is downsampled to 224\u00d7224 (standard VFM input size)</li> </ul>"},{"location":"unreal-scene-generation/#4-get-ground-truth","title":"4. Get Ground Truth","text":"<p>Extract spatial relation labels:</p> <ul> <li>Calculate angular relationship between source and target objects</li> <li>Apply viewpoint transformation (camera for ego, human for allo)</li> <li>Filter ambiguous configurations (objects near decision boundaries)</li> <li>Assign discrete label: Front, Back, Left, or Right</li> </ul>"},{"location":"unreal-scene-generation/#geometric-ambiguity-control","title":"Geometric Ambiguity Control","text":"<p>A key challenge in spatial classification is defining precise boundaries between classes. SpaRRTa implements strict rejection sampling to eliminate label noise:</p> <p> </p> Visualization of valid placement zones and ambiguity exclusion zones (red/gray). Gray zones are the ambiguity exclusion zone for Camera Viewpoint. Red zones are the ambiguity exclusion zone for Human Viewpoint."},{"location":"unreal-scene-generation/#exclusion-zones","title":"Exclusion Zones","text":"<p>Ambiguity zones are defined as conical regions centered along the diagonals:</p> <ul> <li>45\u00b0, 135\u00b0, 225\u00b0, 315\u00b0 relative to the viewpoint's forward vector</li> <li>Any sample where the target falls within these zones is automatically rejected</li> <li>This guarantees unambiguous ground-truth labels</li> </ul> <p>Rejection Sampling</p> <p>The pipeline automatically discards configurations where the target object lies within \u00b115\u00b0 of a diagonal boundary, ensuring all retained samples have mathematically precise labels.</p>"},{"location":"unreal-scene-generation/#technical-implementation","title":"Technical Implementation","text":""},{"location":"unreal-scene-generation/#rendering-stack","title":"Rendering Stack","text":"Component Details Engine Unreal Engine 5.5 Lighting Lumen (dynamic global illumination) Geometry Nanite (virtualized geometry) API Python Editor API + UnrealCV Hardware 2\u00d7 NVIDIA RTX 2080 Ti (11GB VRAM)"},{"location":"unreal-scene-generation/#camera-configuration","title":"Camera Configuration","text":"<p>The camera system uses Unreal Engine's <code>CineCameraActor</code> with standardized settings:</p> <pre><code>def create_camera():\n    \"\"\"\n    Initialize CineCameraActor with standardized parameters.\n    \"\"\"\n    # Filmback settings (sensor dimensions)\n    sensor_width = 50.0   # mm\n    sensor_height = 50.0  # mm\n\n    # Lens settings\n    focal_length = 50.0   # mm\n    aperture = 10.0      # f-stop\n\n    # Calculate FOV from sensor and focal length\n    horizontal_fov = 2 * arctan(0.5 * sensor_width / focal_length)\n    vertical_fov = 2 * arctan(0.5 * sensor_height / focal_length)\n    # Result: ~53\u00b0 horizontal FOV\n    fov = (horizontal_fov, vertical_fov)\n\n    # Create camera with these settings\n    camera = CineCameraActor(location, rotation)\n    camera.set_filmback(sensor_width, sensor_height)\n    camera.set_focal_length(focal_length)\n    camera.set_aperture(aperture)\n\n    return camera\n</code></pre>"},{"location":"unreal-scene-generation/#object-placement-algorithm","title":"Object Placement Algorithm","text":"<p>Objects are placed using an iterative rejection sampling approach with physics-aware terrain adaptation:</p> <pre><code>def place_objects(environment, objects, max_attempts=10):\n    \"\"\"\n    Place objects with physics-aware terrain adaptation.\n    Uses iterative rejection sampling to ensure valid configurations.\n    \"\"\"\n    # Sample center point for object cluster\n    center_x = sample_gaussian(environment_center_x, sample_radius)\n    center_y = sample_gaussian(environment_center_y, sample_radius)\n    center_z = environment_base_z\n\n    for attempt in range(max_attempts):\n        valid_placement = True\n\n        for obj in objects:\n            # Sample random rotation\n            rotation = random_rotation(yaw_range=(0, 360))\n\n            # Sample X, Y positions around center using Gaussian distribution\n            obj_x = random_gaussian(center_x, object_proximity_std)\n            obj_y = random_gaussian(center_y, object_proximity_std)\n\n            # Perform line trace to find ground height at (X, Y)\n            ground_z = detect_ground_at_position(obj_x, obj_y, center_z)\n\n            # Calculate proper Z position accounting for object bounds\n            # Get how much object extends below its origin\n            object_ground_offset = obj.get_ground_offset()\n\n            # Place object so bottom surface touches ground\n            spawn_z = ground_z - object_ground_offset + safety_margin\n\n            position = Vector(obj_x, obj_y, spawn_z)\n            obj.move_to(position, rotation)\n\n        # Validate configuration\n        for i in range(len(objects)):\n            for j in range(i + 1, len(objects)):\n                # Check for AABB overlap (collision detection)\n                if objects[i].overlaps(objects[j]):\n                    valid_placement = False\n                    break\n\n                # Check objects are not too far apart\n                if objects[i].distance_to(objects[j]) &gt; max_distance:\n                    valid_placement = False\n                    break\n\n            if not valid_placement:\n                break\n\n        if valid_placement:\n            return True, (center_x, center_y)\n\n    # All attempts failed\n    return False, (center_x, center_y)\n</code></pre>"},{"location":"unreal-scene-generation/#camera-sampling-with-screenshot","title":"Camera Sampling with Screenshot","text":"<p>Camera positioning uses iterative sampling to ensure all objects are properly framed:</p> <pre><code>def sample_camera(camera, objects, object_center, max_attempts=15):\n    \"\"\"\n    Sample camera position and orientation with validation.\n    Ensures all objects are visible and properly framed.\n    \"\"\"\n    center_x, center_y = object_center\n\n    # Calculate average Z position of all objects\n    avg_object_z = mean([obj.get_location().z for obj in objects])\n\n    for attempt in range(max_attempts):\n        # Sample camera position around object cluster\n        camera_x = random_uniform(center_x - camera_range, center_x + camera_range)\n        camera_y = random_uniform(center_y - camera_range, center_y + camera_range)\n        camera_z = random_uniform(avg_object_z, avg_object_z + camera_height_range)\n\n        camera_position = Vector(camera_x, camera_y, camera_z)\n        camera.move_to(camera_position)\n\n        # Orient camera to look at centroid of all objects\n        object_centroid = calculate_centroid([obj.get_location() for obj in objects])\n        camera.look_at_many(objects)\n\n        # Validate all objects are within camera FOV\n        object_angles = []\n        for obj in objects:\n            # Calculate angle between camera forward vector and object\n            angle = camera.angle_to(obj)\n            object_angles.append(angle)\n\n        max_angle = max(object_angles)\n        min_angle = min(object_angles)\n\n        # Check objects are within FOV bounds\n        # Reject if any object is too far from center (outside FOV)\n        if max_angle &gt; (camera.fov - margin):\n            continue  # Reject and resample\n\n        # Reject if all objects are too close to center (too clustered)\n        if max_angle &lt; min_angle_threshold:\n            continue  # Reject and resample\n\n        # Valid camera configuration found\n        return True\n\n    # All attempts failed\n    return False\n</code></pre> <p>Iterative Rejection Sampling</p> <p>The pipeline uses iterative rejection sampling with configurable maximum attempts:</p> <ul> <li>Object placement: Up to X attempts to find valid non-overlapping configurations</li> <li>Camera sampling: Up to Y attempts to find valid camera positions with all objects in frame</li> <li>Failed attempts are automatically discarded and resampled</li> </ul> <p>Parameter Serialization</p> <p>For each successfully generated scene, the pipeline serializes:</p> <ul> <li>Camera intrinsics (sensor dimensions, focal length, FOV, aperture)</li> <li>Camera extrinsics (position, rotation)</li> <li>Object positions and rotations</li> <li>All metadata saved to JSON files for reproducibility</li> </ul>"},{"location":"unreal-scene-generation/#dataset-statistics","title":"Dataset Statistics","text":"Environment Ego Images Allo Images Forest 5,000 10,000 Desert 5,000 10,000 Winter Town 5,000 10,000 Bridge 5,000 10,000 City 5,000 10,000 Total 25,000 50,000 <p>Dataset Size Rationale</p> <ul> <li>Egocentric: 5,000 images sufficient for generalization</li> <li>Allocentric: 10,000 images needed due to increased task complexity (perspective transformation learning)</li> </ul>"},{"location":"unreal-scene-generation/#scene-visualizations","title":"Scene Visualizations","text":"<p>Explore interactive visualizations of generated scenes showing photorealistic renderings alongside their 3D spatial annotations and 2D top-down views.</p> 1 / 60"},{"location":"unreal-scene-generation/#environment-asset-relations","title":"Environment-Asset Relations","text":"<p>Each environment contains 3 unique object triples used for evaluation. The table below shows the complete mapping of environments to their source objects, target objects, and viewpoint configurations:</p> Triple ID Source Object Target Object Viewpoint Bridge-1 Truck Tree Camera / Human 1 Bridge-2 Bike Trash Bin Camera / Human 2 Bridge-3 Vespa Trash Bin Camera / Human 3 City-1 Vespa Cone Camera / Human 1 City-2 Taxi Fire Hydrant Camera / Human 2 City-3 Bike Cone Camera / Human 3 Desert-1 Truck Rock Camera / Human 1 Desert-2 Camel Cactus Camera / Human 2 Desert-3 Camel Barrel Camera / Human 3 Forest-1 Tree Rock Camera / Human 1 Forest-2 Bear Tent Camera / Human 2 Forest-3 Fox Rock Camera / Human 3 Winter-1 Truck Tree Camera / Human 1 Winter-2 Husky Snowman Camera / Human 2 Winter-3 Deer Tree Camera / Human 3 <p>Viewpoint Configuration</p> <ul> <li>Camera: Used for egocentric (SpaRRTa-ego) task evaluation</li> <li>Human 1 / 2 / 3: Different human models used for allocentric (SpaRRTa-allo) task evaluation, each with unique poses and positions</li> </ul> Next: Evaluation of VFMs \u2192"},{"location":"user-guide/","title":"User Guide","text":""},{"location":"user-guide/#user-guide","title":"User Guide","text":"<p>This guide provides detailed documentation on using SpaRRTa for evaluating spatial reasoning in Visual Foundation Models.</p> <p>Coming Soon</p> <p>This section is currently under development. Full documentation will be available soon.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#blog","title":"Blog","text":"<p>Stay updated with the latest news, releases, and insights from the SpaRRTa project.</p>"},{"location":"blog/welcome-to-sparrta/","title":"Welcome to SpaRRTa","text":"","tags":["release","benchmark","vfm"]},{"location":"blog/welcome-to-sparrta/#welcome-to-sparrta","title":"Welcome to SpaRRTa","text":"<p>We're excited to announce SpaRRTa (Spatial Relation Recognition Task), a new synthetic benchmark for evaluating spatial intelligence in Visual Foundation Models.</p>","tags":["release","benchmark","vfm"]},{"location":"blog/welcome-to-sparrta/#what-is-sparrta","title":"What is SpaRRTa?","text":"<p>SpaRRTa is a benchmark designed to evaluate how Visual Foundation Models (VFMs) encode and represent spatial relations between objects. Unlike traditional 3D benchmarks that focus on explicit metric predictions like depth estimation, SpaRRTa targets abstract, human-like relational spatial reasoning.</p>","tags":["release","benchmark","vfm"]},{"location":"blog/welcome-to-sparrta/#key-features","title":"Key Features","text":"<ul> <li>Photorealistic Synthetic Data: Built with Unreal Engine 5 for high-fidelity images</li> <li>Diverse Environments: 5 distinct environments from sparse deserts to dense urban scenes</li> <li>Two Task Variants: Egocentric (camera-view) and Allocentric (perspective-taking) tasks</li> <li>Comprehensive Evaluation: Support for multiple probing strategies</li> </ul>","tags":["release","benchmark","vfm"]},{"location":"blog/welcome-to-sparrta/#why-sparrta","title":"Why SpaRRTa?","text":"<p>Visual Foundation Models have demonstrated remarkable performance in semantic understanding, but their spatial reasoning capabilities remain understudied. SpaRRTa provides a systematic way to evaluate this critical capability, which is essential for embodied AI applications.</p>","tags":["release","benchmark","vfm"]},{"location":"blog/welcome-to-sparrta/#getting-started","title":"Getting Started","text":"<p>Check out our Getting Started Guide to begin using SpaRRTa in your research.</p>","tags":["release","benchmark","vfm"]},{"location":"blog/welcome-to-sparrta/#citation","title":"Citation","text":"<p>If you use SpaRRTa in your research, please cite our paper:</p> <pre><code>@misc{kargin2026sparrta,\n      title={SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models}, \n      author={Turhan Can Kargin and Wojciech Jasi\u0144ski and Adam Pardyl and Bartosz Zieli\u0144ski and Marcin Przewi\u0119\u017alikowski},\n      year={2026},\n      eprint={2601.11729},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2601.11729}, \n}\n</code></pre> <p>Stay tuned for more updates!</p>","tags":["release","benchmark","vfm"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2025/#2025","title":"2025","text":""},{"location":"blog/category/announcements/","title":"Announcements","text":""},{"location":"blog/category/announcements/#announcements","title":"Announcements","text":""}]}